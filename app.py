import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import warnings
warnings.filterwarnings("ignore")

"""
Sarashinaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸGradioãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
Hugging Face Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œ
"""

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
MODEL_NAME = "sbintuitions/sarashina2.2-3b-instruct-v0.1"

print("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None,
    trust_remote_code=True
)
print("ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

def respond(
    message,
    history: list[tuple[str, str]],
    system_message,
    max_tokens,
    temperature,
    top_p,
):
    """
    ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    Gradio ChatInterfaceã®æ¨™æº–å½¢å¼ã«å¯¾å¿œ
    """
    try:
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é©åˆ‡ãªå‹ã«å¤‰æ›ï¼ˆAPIå‘¼ã³å‡ºã—æ™‚ã®æ–‡å­—åˆ—å¯¾ç­–ï¼‰
        try:
            max_tokens = int(max_tokens) if max_tokens is not None else 512
            # max_tokensãŒ0ä»¥ä¸‹ã®å ´åˆã¯512ã«è¨­å®š
            if max_tokens <= 0:
                max_tokens = 1
        except (ValueError, TypeError):
            max_tokens = 512
            
        try:
            temperature = float(temperature) if temperature is not None else 0.7
            # temperatureãŒ0ä»¥ä¸‹ã®å ´åˆã¯0.7ã«è¨­å®š
            if temperature <= 0:
                temperature = 0.7
        except (ValueError, TypeError):
            temperature = 0.7
            
        try:
            top_p = float(top_p) if top_p is not None else 0.95
            # top_pãŒ0ä»¥ä¸‹ã®å ´åˆã¯0.95ã«è¨­å®š
            if top_p <= 0:
                top_p = 0.95
        except (ValueError, TypeError):
            top_p = 0.95
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¼šè©±å±¥æ­´ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        conversation = ""
        if system_message and system_message.strip():
            conversation += f"ã‚·ã‚¹ãƒ†ãƒ : {system_message}\n"
        
        # ä¼šè©±å±¥æ­´ã‚’è¿½åŠ 
        for user_msg, bot_msg in history:
            if user_msg:
                conversation += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_msg}\n"
            if bot_msg:
                conversation += f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {bot_msg}\n"
        
        # ç¾åœ¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        conversation += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {message}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: "
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        inputs = tokenizer.encode(conversation, return_tensors="pt")
        
        # GPUä½¿ç”¨æ™‚ã¯CUDAã«ç§»å‹•
        if torch.cuda.is_available():
            inputs = inputs.cuda()
        
        # å¿œç­”ç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰
        response = ""
        with torch.no_grad():
            # ä¸€åº¦ã«ç”Ÿæˆã—ã¦ã‹ã‚‰ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é¢¨ã«å‡ºåŠ›
            outputs = model.generate(
                inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # å¿œç­”éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
        full_response = generated[len(conversation):].strip()
        
        # ä¸è¦ãªéƒ¨åˆ†ã‚’é™¤å»
        if "ãƒ¦ãƒ¼ã‚¶ãƒ¼:" in full_response:
            full_response = full_response.split("ãƒ¦ãƒ¼ã‚¶ãƒ¼:")[0].strip()
        
        return full_response
            
            
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

"""
Gradio ChatInterfaceã‚’ä½¿ç”¨ã—ãŸã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å«ã‚€
"""
demo = gr.ChatInterface(
    respond,
    title="ğŸ¤– Sarashina Chatbot",
    description="Sarashina2.2-3b-instruct ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚",
    additional_inputs=[
        gr.Textbox(
            value="ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯Œãªæ—¥æœ¬èªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚", 
            label="ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
            lines=3
        ),
        gr.Slider(
            minimum=1, 
            maximum=1024, 
            value=512, 
            step=1, 
            label="æœ€å¤§æ–°è¦ãƒˆãƒ¼ã‚¯ãƒ³æ•°"
        ),
        gr.Slider(
            minimum=0.1, 
            maximum=2.0, 
            value=0.7, 
            step=0.1, 
            label="Temperature (å‰µé€ æ€§)"
        ),
        gr.Slider(
            minimum=0.1,
            maximum=1.0,
            value=0.95,
            step=0.05,
            label="Top-p (å¤šæ§˜æ€§åˆ¶å¾¡)",
        ),
    ],
    theme=gr.themes.Soft(),
    examples=[
        ["ã“ã‚“ã«ã¡ã¯ï¼ä»Šæ—¥ã¯ã©ã‚“ãªã“ã¨ã‚’è©±ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ"],
        ["æ—¥æœ¬ã®æ–‡åŒ–ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"],
        ["ç°¡å˜ãªãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦ã‚‚ã‚‰ãˆã¾ã™ã‹ï¼Ÿ"],
        ["ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«ã¤ã„ã¦è³ªå•ãŒã‚ã‚Šã¾ã™ã€‚"],
    ],
    cache_examples=False,
)

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_api=True,  # API documentation ã‚’è¡¨ç¤º
        debug=True
    )